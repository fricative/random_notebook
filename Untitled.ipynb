{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last December, I set up a scraping script on AWS Lambda to automatically scrape headlines from financial news websites (e.g. Reuters, CNBC) every 3 hours. Using NLP on news data are now a popular topic in finance. Two Sigma is holding Kaggle competitions to use news to predict stock returns. I think it should be interesting to do some exploratory ML on the news I have scraped so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the news headline CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5765</th>\n",
       "      <td>reuters</td>\n",
       "      <td>2018-12-07 10:00:48.453100</td>\n",
       "      <td>[\"Huawei CFO to appear in Canada court\", \"A to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5763</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>2018-12-07 10:00:48.453100</td>\n",
       "      <td>[\"Tech stocks lift European markets; oil price...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5764</th>\n",
       "      <td>ft</td>\n",
       "      <td>2018-12-07 10:00:48.453100</td>\n",
       "      <td>[\"Top stories\", \"Cyber Security\", \"Huawei cave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5822</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>2018-12-07 11:00:48.314326</td>\n",
       "      <td>[\"European markets rally after global sell-off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823</th>\n",
       "      <td>ft</td>\n",
       "      <td>2018-12-07 11:00:48.314326</td>\n",
       "      <td>[\"Top stories\", \"Cyber Security\", \"Huawei cave...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source                  timestamp  \\\n",
       "5765  reuters 2018-12-07 10:00:48.453100   \n",
       "5763     cnbc 2018-12-07 10:00:48.453100   \n",
       "5764       ft 2018-12-07 10:00:48.453100   \n",
       "5822     cnbc 2018-12-07 11:00:48.314326   \n",
       "5823       ft 2018-12-07 11:00:48.314326   \n",
       "\n",
       "                                               headline  \n",
       "5765  [\"Huawei CFO to appear in Canada court\", \"A to...  \n",
       "5763  [\"Tech stocks lift European markets; oil price...  \n",
       "5764  [\"Top stories\", \"Cyber Security\", \"Huawei cave...  \n",
       "5822  [\"European markets rally after global sell-off...  \n",
       "5823  [\"Top stories\", \"Cyber Security\", \"Huawei cave...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('~/Downloads/headlines.csv', parse_dates=['timestamp'], index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peeking into what how our raw text data look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"Tech stocks lift European markets; oil prices slide ahead of OPEC meeting\", \"Asian stocks take a breather after days of declines\", \"Dow set to fall by 200 points at the open as sell-off continues\", \"Gold set for best week since August, US jobs data eyed\", \"Oil sinks as OPEC mulls Iran supply cut exemption, tries to get Russia on board\", \"Treasury yields muted ahead of nonfarm payrolls\", \"Dollar struggles on Fed pause talk ahead of jobs data\", \"Dollar struggles on Fed pause talk ahead of jobs data\", \"Bitcoin/USD Coinbase\", \"Iran seeks exemptions as OPEC awaits approval from Russia to impose production cuts\", \"A crunch Brexit vote is coming next week that could plunge the UK into fresh political chaos\", \"2 Hours Ago\", \"Bitcoin plunges 11 percent as December rout continues\", \"Eustance Huang\", \"3 Hours Ago\", \"John Bolton \\'knew in advance\\' about arrest of Huawei executive; official says Trump did not\", \"Dow rebounds from 780-point plunge, ends day just slightly lower on report Fed may pause hikes\", \"Christine Wang, Fred Imbert, Eustance Huang\", \"Facebook emails show EU must toughen regulation on social media \\'monopoly,\\' lawmaker says\", \"Ryan Browne\", \"56 Mins Ago\", \"Oil sinks as OPEC mulls Iran supply cut exemption, tries to get Russia on board\", \"19 Mins Ago\", \"How cable-car systems are revolutionizing public transport almost 12,000 feet above sea level\", \"Anmar Frangoul\", \"42 Mins Ago\", \"Anti-election meddling group makes A.I.-powered Trump impersonator to warn about \\'deepfakes\\'\", \"Ryan Browne\", \"2 Hours Ago\", \"93% of funding for European tech start-ups goes to all-male teams, new report finds\", \"Elizabeth Schulze\", \"3 Hours Ago\", \"Germany is about to find out who\\'s likely to replace Merkel as leader\", \"Holly Ellyatt\", \"Huawei appoints chairman as its acting CFO after executive\\'s arrest\", \"Matt Clinch\", \"2 Hours Ago\", \"How this 32-year-old high school dropout created Hong Kong\\'s first billion-dollar start-up\", \"Karen Gilchrist\", \"Japan said to plan to bar Huawei, ZTE from government procurement contracts\", \"Soul-searching is in order for Nissan\\'s board after Ghosn allegations, governance experts say\", \"Kelly Olsen\", \"3 Hours Ago\", \"Reid Hoffman on starting LinkedIn, how to scale a business and his life\\'s philosophy\", \"Lucy Handley\", \"HSBC ties to Huawei could complicate China trade talks even though the bank isn\\'t being investigated\", \"Kate Fazzini\", \"India and Pakistan turn to religious diplomacy as peace talks stall\", \"Nyshka Chandran\", \"South Korean peace efforts look \\'out of sync\\' with elimination of North Korean nukes\", \"Nyshka Chandran\", \"Markets are going haywire. Here\\'s why these sudden moves are here to stay\", \"Jamie Dimon says the market is getting ripped around thanks to the trade war\", \"Trump seen naming former Fox News personality as next UN ambassador\", \"Tesla General Counsel Todd Maron is leaving the company\", \"Lora Kolodny\", \"3 Hours Ago\", \"Asian stocks take a breather after days of declines\", \"Eustance Huang\", \"2 Hours Ago\", \"Police raids were not the fault of Deutsche Bank management, CFO says\", \"Elon Musk says Boring Company tunnel under LA will now open on Dec. 18\", \"Yen Nee Lee\", \"Tariffs have hit confidence, to slow US economy, says Fed\\'s Williams\", \"OPEC meeting ends with no decision on production levels\", \"Sam Meredith, Tom DiChristopher\", \"Noble Group\\'s $3.5 billion restructuring at risk as authorities block new listing\", \"Israel\\'s Lebanon border operation seen as a political move by embattled Netanyahu\", \"Natasha Turak\", \"The sell-off started with a mysterious plunge overnight that caused the exchange to halt futures\", \"Thomas Franck\", \"US crude sinks 2.7%, settling at $51.49, after OPEC delays decision on production cut levels\", \"Tom DiChristopher, Sam Meredith, Matt Clinch\", \"These are the 10 best firms to work for in France, according to Glassdoor\", \"Alexandra Gibbs\", \"US firms doing business in China now vulnerable after Huawei arrest: Former US trade advisor\", \"Michelle Fox\", \"There may be more at stake than just trade concessions in the US-China tariff battle\", \"Nyshka Chandran\", \"Shares of Apple suppliers crumble after lensmaker reports more than 25 percent decline in revenue\", \"Eustance Huang\", \"Sell-offs could be down to machines that control 80% of the US stock market, fund manager says\", \"Silvia Amaro\", \"Follow CNBC International\", \"Market MOVERS\"]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['headline'].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty raw, this cell contains all the headlines from one news source (e.g. CNBC) at a specific timestamp. Multiple news list are stored as a JSON string. Next step is to do some cleaning on the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = data['source'].tolist()\n",
    "timestamp = data['timestamp'].tolist()\n",
    "headline = data['headline'].tolist()\n",
    "\n",
    "expanded_source = []\n",
    "expanded_timestamp = []\n",
    "expanded_headline = []\n",
    "\n",
    "# in the original dataframe, each row contains multiple headlines as shown above.\n",
    "# we need to separate them out into one headline per row.\n",
    "\n",
    "for i, hl in enumerate(headline):\n",
    "    hl_list = json.loads(hl)\n",
    "    for j in range(len(hl_list)):\n",
    "        \n",
    "        # removing all texts that are not really headlines\n",
    "        # noticed we have some news headline string that are not\n",
    "        # really a headline, e.g. \"2 Hours Ago\", \"Kelly Olsen\". \n",
    "        # these are just irrelevant metadata scraped from the website.\n",
    "        # It is easy to notice all such text data are 30 characters\n",
    "        # or less, hence we could simply filter them out by string length.\n",
    "        \n",
    "        if len(hl_list[j]) < 30:\n",
    "            continue\n",
    "        expanded_source.append(source[i])\n",
    "        expanded_timestamp.append(timestamp[i])\n",
    "        expanded_headline.append(hl_list[j])\n",
    "\n",
    "data = pd.DataFrame({'source': expanded_source, 'timestamp': expanded_timestamp,\n",
    "                     'headline': expanded_headline})\n",
    "\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# preprocessing pipeline to clean the text\n",
    "def text_preprocess(text, return_str=True):\n",
    "    table = str.maketrans('', '', '!\"#&\\'()*+,./:;<=>?@[\\\\]^_`{|}~‘’')\n",
    "    text = word_tokenize(text)\n",
    "    text = [t.translate(table) for t in text]\n",
    "    text = [t for t in text if t not in stop_words]\n",
    "    text = [lemmatiser.lemmatize(t) for t in text]\n",
    "    text = [stemmer.stem(t) for t in text]\n",
    "    text = [t for t in text if t != '']\n",
    "    return ' '.join(text) if return_str else text\n",
    "\n",
    "data['processed'] = data['headline'].apply(text_preprocess)\n",
    "data['date'] = data['timestamp'].apply(lambda x: x.date().isoformat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load stock index data for the past 7 months period. I downloaded both S&P 500 and MSCI World Indices. For this exercise, I will focus on MSCI, result on SPX are quite similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>spx</th>\n",
       "      <th>mxwo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>2760.17</td>\n",
       "      <td>2041.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-02</td>\n",
       "      <td>2760.17</td>\n",
       "      <td>2041.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-03</td>\n",
       "      <td>2790.37</td>\n",
       "      <td>2066.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-04</td>\n",
       "      <td>2700.06</td>\n",
       "      <td>2016.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-05</td>\n",
       "      <td>2700.06</td>\n",
       "      <td>2008.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date      spx     mxwo\n",
       "0 2018-12-01  2760.17  2041.36\n",
       "1 2018-12-02  2760.17  2041.36\n",
       "2 2018-12-03  2790.37  2066.62\n",
       "3 2018-12-04  2700.06  2016.89\n",
       "4 2018-12-05  2700.06  2008.63"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_data = pd.read_csv('~/Downloads/index.csv', parse_dates=['date'])\n",
    "index_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data contained weekend as well. We filter them out.\n",
    "\n",
    "index_data['weekday'] = index_data['date'].apply(lambda x: x.isoweekday() <= 5)\n",
    "index_data = index_data[index_data['weekday']]\n",
    "del index_data['weekday']\n",
    "\n",
    "# use MSCI World Index 1 day return as the indicator for market sentiment\n",
    "index = 'mxwo'\n",
    "target = 'mxwo_1d_ret'\n",
    "\n",
    "index_data[target] = np.log(index_data[index + ''].shift(-1) / index_data[index + ''])\n",
    "index_data.dropna(inplace=True)\n",
    "\n",
    "# helper function to convert a daily return into a label within \n",
    "# each label represents a 1% return interval\n",
    "def generate_label(dataframe, column, interval=.01):\n",
    "    def categorize(num):\n",
    "        cat = (math.ceil(abs(num) / interval) ) * (1 if num > 0 else -1)\n",
    "        # winsorize the data to (+/-)2% interval to \n",
    "        # prevent having outlier classes that\n",
    "        # only show up in training set or test set\n",
    "        return cat if abs(cat) <= 2 else 2 if cat > 0 else -2\n",
    "    dataframe[column] = dataframe[column].apply(categorize)\n",
    "\n",
    "generate_label(index_data, target)\n",
    "index_data['date'] = index_data['date'].apply(lambda x: x.date().isoformat())\n",
    "\n",
    "# merge headline dataframe with index data. Note weekend news are \n",
    "# dropped in inner join for simplicity.\n",
    "data = data.merge(index_data, on='date', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>headline</th>\n",
       "      <th>processed</th>\n",
       "      <th>date</th>\n",
       "      <th>spx</th>\n",
       "      <th>mxwo</th>\n",
       "      <th>mxwo_1d_ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reuters</td>\n",
       "      <td>2018-12-07 10:00:48.453100</td>\n",
       "      <td>Huawei CFO to appear in Canada court</td>\n",
       "      <td>huawei cfo appear canada court</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>2633.08</td>\n",
       "      <td>1965.24</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reuters</td>\n",
       "      <td>2018-12-07 10:00:48.453100</td>\n",
       "      <td>A top executive of China's Huawei Technologies...</td>\n",
       "      <td>A top execut china huawei technolog arrest can...</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>2633.08</td>\n",
       "      <td>1965.24</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reuters</td>\n",
       "      <td>2018-12-07 10:00:48.453100</td>\n",
       "      <td>Huawei appoints chairman as acting CFO</td>\n",
       "      <td>huawei appoint chairman act cfo</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>2633.08</td>\n",
       "      <td>1965.24</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reuters</td>\n",
       "      <td>2018-12-07 10:00:48.453100</td>\n",
       "      <td>Japan to shun Huawei, ZTE equipment</td>\n",
       "      <td>japan shun huawei zte equip</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>2633.08</td>\n",
       "      <td>1965.24</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reuters</td>\n",
       "      <td>2018-12-07 10:00:48.453100</td>\n",
       "      <td>TV: Trump was unaware of arrest - officials</td>\n",
       "      <td>TV trump unawar arrest - offici</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>2633.08</td>\n",
       "      <td>1965.24</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source                  timestamp  \\\n",
       "0  reuters 2018-12-07 10:00:48.453100   \n",
       "1  reuters 2018-12-07 10:00:48.453100   \n",
       "2  reuters 2018-12-07 10:00:48.453100   \n",
       "3  reuters 2018-12-07 10:00:48.453100   \n",
       "4  reuters 2018-12-07 10:00:48.453100   \n",
       "\n",
       "                                            headline  \\\n",
       "0               Huawei CFO to appear in Canada court   \n",
       "1  A top executive of China's Huawei Technologies...   \n",
       "2             Huawei appoints chairman as acting CFO   \n",
       "3                Japan to shun Huawei, ZTE equipment   \n",
       "4        TV: Trump was unaware of arrest - officials   \n",
       "\n",
       "                                           processed        date      spx  \\\n",
       "0                     huawei cfo appear canada court  2018-12-07  2633.08   \n",
       "1  A top execut china huawei technolog arrest can...  2018-12-07  2633.08   \n",
       "2                    huawei appoint chairman act cfo  2018-12-07  2633.08   \n",
       "3                        japan shun huawei zte equip  2018-12-07  2633.08   \n",
       "4                    TV trump unawar arrest - offici  2018-12-07  2633.08   \n",
       "\n",
       "      mxwo  mxwo_1d_ret  \n",
       "0  1965.24           -1  \n",
       "1  1965.24           -1  \n",
       "2  1965.24           -1  \n",
       "3  1965.24           -1  \n",
       "4  1965.24           -1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176721, 11)\n",
      "2018-12-07 10:00:48.453100\n",
      "2019-07-08 21:00:00\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data['timestamp'].min())\n",
    "print(data['timestamp'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "176721 headlines for the period from 2018-12-7 to 2019-7-8 captured. I will sample 30000 news for the purpose of this exercise. 20000 will be used for training and the remaining 10000 used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = np.random.choice(data.shape[0], size=30000, replace=False)\n",
    "train = data.loc[sample_index[:20000], :]\n",
    "test = data.loc[sample_index[20000:], :]\n",
    "\n",
    "# sampling without look-ahead bias\n",
    "# train = data.iloc[:100000, :].sample(10000)\n",
    "# test = data.iloc[100000:, :].sample(10000)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "# fit tokenizer on the whole corpus of text. Maybe restrict this to \n",
    "# training set corpus to avoid look-ahead bias?\n",
    "tokenizer.fit_on_texts(data['processed'].tolist())\n",
    "\n",
    "train_x = train['processed'].tolist()\n",
    "train_x = tokenizer.texts_to_sequences(train_x)\n",
    "headline_len = [len(vector) for vector in train_x]\n",
    "max_len = int(np.percentile(headline_len, 80))         # take the 80th percentile length\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "train_x = pad_sequences(train_x, maxlen=max_len, padding='pre', truncating='post')\n",
    "train_y = to_categorical(train[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of training, I will use a LSTM RNN to do this classification exercise. Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 16s - loss: 0.8435 - acc: 0.5971\n",
      "Epoch 2/30\n",
      " - 14s - loss: 0.6495 - acc: 0.6447\n",
      "Epoch 3/30\n",
      " - 13s - loss: 0.5586 - acc: 0.7276\n",
      "Epoch 4/30\n",
      " - 14s - loss: 0.4821 - acc: 0.7772\n",
      "Epoch 5/30\n",
      " - 14s - loss: 0.4246 - acc: 0.8114\n",
      "Epoch 6/30\n",
      " - 14s - loss: 0.3767 - acc: 0.8384\n",
      "Epoch 7/30\n",
      " - 13s - loss: 0.3329 - acc: 0.8572\n",
      "Epoch 8/30\n",
      " - 14s - loss: 0.3019 - acc: 0.8699\n",
      "Epoch 9/30\n",
      " - 14s - loss: 0.2771 - acc: 0.8800\n",
      "Epoch 10/30\n",
      " - 14s - loss: 0.2509 - acc: 0.8930\n",
      "Epoch 11/30\n",
      " - 13s - loss: 0.2350 - acc: 0.8993\n",
      "Epoch 12/30\n",
      " - 14s - loss: 0.2159 - acc: 0.9073\n",
      "Epoch 13/30\n",
      " - 14s - loss: 0.2017 - acc: 0.9115\n",
      "Epoch 14/30\n",
      " - 13s - loss: 0.1906 - acc: 0.9153\n",
      "Epoch 15/30\n",
      " - 13s - loss: 0.1830 - acc: 0.9187\n",
      "Epoch 16/30\n",
      " - 14s - loss: 0.1717 - acc: 0.9230\n",
      "Epoch 17/30\n",
      " - 13s - loss: 0.1664 - acc: 0.9229\n",
      "Epoch 18/30\n",
      " - 14s - loss: 0.1601 - acc: 0.9257\n",
      "Epoch 19/30\n",
      " - 14s - loss: 0.1569 - acc: 0.9232\n",
      "Epoch 20/30\n",
      " - 14s - loss: 0.1503 - acc: 0.9284\n",
      "Epoch 21/30\n",
      " - 14s - loss: 0.1481 - acc: 0.9286\n",
      "Epoch 22/30\n",
      " - 14s - loss: 0.1433 - acc: 0.9309\n",
      "Epoch 23/30\n",
      " - 14s - loss: 0.1427 - acc: 0.9308\n",
      "Epoch 24/30\n",
      " - 14s - loss: 0.1367 - acc: 0.9304\n",
      "Epoch 25/30\n",
      " - 13s - loss: 0.1357 - acc: 0.9317\n",
      "Epoch 26/30\n",
      " - 14s - loss: 0.1334 - acc: 0.9329\n",
      "Epoch 27/30\n",
      " - 15s - loss: 0.1314 - acc: 0.9331\n",
      "Epoch 28/30\n",
      " - 14s - loss: 0.1322 - acc: 0.9335\n",
      "Epoch 29/30\n",
      " - 13s - loss: 0.1284 - acc: 0.9343\n",
      "Epoch 30/30\n",
      " - 14s - loss: 0.1265 - acc: 0.9353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a8fce4828>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 500\n",
    "rnn = Sequential()\n",
    "rnn.add(Embedding(vocabulary_size, embedding_dim, input_length=max_len))\n",
    "rnn.add(SpatialDropout1D(0.2))\n",
    "rnn.add(LSTM(units=30, dropout=.2))\n",
    "rnn.add(Dense(train_y.shape[1], activation='softmax'))\n",
    "rnn.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "rnn.fit(train_x, train_y, batch_size=512, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training stats, looks like the process has converged successfully. Let's see how\n",
    "it performs on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headline level accuracy score 0.712\n"
     ]
    }
   ],
   "source": [
    "# feed test set to feed into RNN\n",
    "test_x = test['processed'].tolist()\n",
    "test_x = tokenizer.texts_to_sequences(test_x)\n",
    "test_x = pad_sequences(test_x, maxlen=max_len, padding='pre', truncating='post')\n",
    "pred_y = rnn.predict(test_x)\n",
    "pred_y = np.argmax(pred_y, axis=1)\n",
    "\n",
    "# RNN's output doesn't use the class label we originally generated. So we \n",
    "# need to convert our generated label to the the class label using to_categorical() \n",
    "test_y = to_categorical(test[target])\n",
    "test_y = np.argmax(test_y, axis=1)\n",
    "\n",
    "headline = pd.DataFrame({'date': test['date'], 'headline': test['headline'], 'actual': test_y, 'predict': pred_y})\n",
    "headline.sort_values('date', inplace=True)\n",
    "\n",
    "# accuracy score on a per headline basis\n",
    "print('headline level accuracy score {}'.format(accuracy_score(headline['actual'], headline['predict'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each headline, the model is able to achieve an accuracy score of 71%. However, not all headlines are high relevance financial news headline. Some headlines are actually ads or articles on topics such as art, sports etc. These websites sometimes display non-financial articles (e.g. art, sport topics) on their front page. In order to come up with a prediction for the next day's return. We make use of all headlines we gathered on a day.\n",
    "\n",
    "Since our the activation function of our last layer is a softmax function which spits out probability value of belonging to each class, we could sum up the probability of each class of all articles for a day, pick the class with the largest value as our predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day level accuracy score 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "pred_y = rnn.predict(test_x)\n",
    "#pred_y = scale(pred_y)\n",
    "pred_y = np.hstack((pred_y, test.loc[:, ['date']]))\n",
    "category_prob = pd.DataFrame(pred_y)\n",
    "category_prob.columns = [0, 1, 2, 'date']\n",
    "category_prob.sort_values('date', inplace=True)\n",
    "\n",
    "sum_prob = category_prob.groupby('date').sum()\n",
    "date = sum_prob.index\n",
    "label = np.argmax(sum_prob.values, axis=1)\n",
    "sum_prob = pd.DataFrame({'date': date, 'predict': label})\n",
    "sum_prob.set_index('date', inplace=True)\n",
    "y_label = headline.groupby('date').mean()['actual']\n",
    "result = sum_prob.join(y_label)\n",
    "\n",
    "# accuracy score on a per day basis\n",
    "print('day level accuracy score {}'.format(accuracy_score(result['actual'], result['predict'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, this drastically improves the accuracy rate to a 94%. That is out of my expectation. Let's have a look at how our prediction looks like on a per-headline basis and daily basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>actual</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114269</th>\n",
       "      <td>2019-03-05</td>\n",
       "      <td>Trump associate Felix Sater accused of hacking...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72740</th>\n",
       "      <td>2019-01-11</td>\n",
       "      <td>Trump weighs declaring emergency</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116869</th>\n",
       "      <td>2019-03-08</td>\n",
       "      <td>LaCroix maker’s CEO compares job to caring for...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101618</th>\n",
       "      <td>2019-02-08</td>\n",
       "      <td>Not taking your full hour for lunch? You're no...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61591</th>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>UK financial services industry moves $1 trilli...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127002</th>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>Top economists worry Fed pick Stephen Moore is...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77118</th>\n",
       "      <td>2019-01-14</td>\n",
       "      <td>Ankara hits back by insisting US allies in Syr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67794</th>\n",
       "      <td>2019-01-09</td>\n",
       "      <td>Forget the US and Asia, the top 5 countries fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90581</th>\n",
       "      <td>2019-01-25</td>\n",
       "      <td>2018 was the planet’s fourth hottest year on r...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168338</th>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>Activists have prompted groups to make placato...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102203</th>\n",
       "      <td>2019-02-08</td>\n",
       "      <td>Desperation at the border as Venezuelans await...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93637</th>\n",
       "      <td>2019-01-29</td>\n",
       "      <td>Bitcoin dips below $3,400, on track for 5th st...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127724</th>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>Low mortgage rates are set to stoke demand amo...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25300</th>\n",
       "      <td>2018-12-20</td>\n",
       "      <td>US and UK accuse China-backed hackers of cyber...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144551</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>Carol Whitmire worked at Walmart for 8 years, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4258</th>\n",
       "      <td>2018-12-12</td>\n",
       "      <td>President bickers with top congressional Democ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27171</th>\n",
       "      <td>2018-12-21</td>\n",
       "      <td>Oil prices rise as OPEC output cuts seen to be...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117635</th>\n",
       "      <td>2019-03-12</td>\n",
       "      <td>Fox News ‘strongly condemns’ anchor Jeanine Pi...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92787</th>\n",
       "      <td>2019-01-29</td>\n",
       "      <td>If your gross income was low enough last year,...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70427</th>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>Jaguar Land Rover to slash 4,500 UK jobs after...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date                                           headline  actual  \\\n",
       "114269  2019-03-05  Trump associate Felix Sater accused of hacking...       2   \n",
       "72740   2019-01-11                   Trump weighs declaring emergency       2   \n",
       "116869  2019-03-08  LaCroix maker’s CEO compares job to caring for...       2   \n",
       "101618  2019-02-08  Not taking your full hour for lunch? You're no...       1   \n",
       "61591   2019-01-07  UK financial services industry moves $1 trilli...       1   \n",
       "127002  2019-03-29  Top economists worry Fed pick Stephen Moore is...       2   \n",
       "77118   2019-01-14  Ankara hits back by insisting US allies in Syr...       1   \n",
       "67794   2019-01-09  Forget the US and Asia, the top 5 countries fo...       1   \n",
       "90581   2019-01-25  2018 was the planet’s fourth hottest year on r...       2   \n",
       "168338  2019-06-20  Activists have prompted groups to make placato...       2   \n",
       "102203  2019-02-08  Desperation at the border as Venezuelans await...       1   \n",
       "93637   2019-01-29  Bitcoin dips below $3,400, on track for 5th st...       2   \n",
       "127724  2019-04-01  Low mortgage rates are set to stoke demand amo...       2   \n",
       "25300   2018-12-20  US and UK accuse China-backed hackers of cyber...       1   \n",
       "144551  2019-05-03  Carol Whitmire worked at Walmart for 8 years, ...       2   \n",
       "4258    2018-12-12  President bickers with top congressional Democ...       2   \n",
       "27171   2018-12-21  Oil prices rise as OPEC output cuts seen to be...       1   \n",
       "117635  2019-03-12  Fox News ‘strongly condemns’ anchor Jeanine Pi...       1   \n",
       "92787   2019-01-29  If your gross income was low enough last year,...       2   \n",
       "70427   2019-01-10  Jaguar Land Rover to slash 4,500 UK jobs after...       2   \n",
       "\n",
       "        predict  \n",
       "114269        2  \n",
       "72740         2  \n",
       "116869        2  \n",
       "101618        1  \n",
       "61591         1  \n",
       "127002        2  \n",
       "77118         1  \n",
       "67794         1  \n",
       "90581         2  \n",
       "168338        1  \n",
       "102203        2  \n",
       "93637         2  \n",
       "127724        1  \n",
       "25300         1  \n",
       "144551        1  \n",
       "4258          2  \n",
       "27171         1  \n",
       "117635        2  \n",
       "92787         1  \n",
       "70427         2  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-04-24</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-10</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-29</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-07</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-19</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-08</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-12</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-24</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-15</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-21</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-02</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-10</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-14</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-07</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-08</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-12</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predict  actual\n",
       "date                       \n",
       "2019-04-24        2     2.0\n",
       "2019-06-10        1     1.0\n",
       "2019-01-29        2     2.0\n",
       "2018-12-07        2     2.0\n",
       "2019-06-19        2     2.0\n",
       "2019-03-08        2     2.0\n",
       "2019-02-12        1     1.0\n",
       "2019-06-24        2     2.0\n",
       "2019-04-15        1     1.0\n",
       "2019-03-21        1     1.0\n",
       "2019-07-02        1     1.0\n",
       "2018-12-10        1     1.0\n",
       "2019-04-01        1     2.0\n",
       "2019-02-14        1     1.0\n",
       "2019-01-07        1     1.0\n",
       "2019-02-08        1     1.0\n",
       "2019-03-12        1     1.0\n",
       "2019-03-18        1     1.0\n",
       "2018-12-31        1     1.0\n",
       "2019-01-22        1     1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result looks impressive. However, don't forget we generate our training and test \n",
    "set using random sampling on data across the entire 7 months period. This to some degree\n",
    "suffered from look-ahead basis. headlines from the same period normally have similar word\n",
    "distribution, in the beginning of Jan 2019, the market saw a wave of rebound. Having seen in training set words like \"surge, rebound, recover\" are associated with a very positive label, it is natural the classifier will link headlines with similar meaning to positive label. \n",
    "\n",
    "In the training/test data generation part,I commented out another way to generate train/test set without look ahead bias. Under this setting, the accuracy rate drops to around 50%! Will\n",
    "longer history help? Since we only have 7 months of data to train on, probably a long 10 year \n",
    "series span across different market sentiment cycle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
