{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last December, I set up a scraping script on AWS Lambda to automatically scrape headlines from financial news websites (e.g. Reuters, CNBC) every 3 hours. Using NLP on news data are now a popular topic in finance. Two Sigma is holding Kaggle competitions to use news to predict stock returns. I think it should be interesting to do some exploratory ML on the news I have scraped so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5765</th>\n",
       "      <td>reuters</td>\n",
       "      <td>2018-12-07 10:00:48.453100</td>\n",
       "      <td>[\"Huawei CFO to appear in Canada court\", \"A to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5763</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>2018-12-07 10:00:48.453100</td>\n",
       "      <td>[\"Tech stocks lift European markets; oil price...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5764</th>\n",
       "      <td>ft</td>\n",
       "      <td>2018-12-07 10:00:48.453100</td>\n",
       "      <td>[\"Top stories\", \"Cyber Security\", \"Huawei cave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5822</th>\n",
       "      <td>cnbc</td>\n",
       "      <td>2018-12-07 11:00:48.314326</td>\n",
       "      <td>[\"European markets rally after global sell-off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823</th>\n",
       "      <td>ft</td>\n",
       "      <td>2018-12-07 11:00:48.314326</td>\n",
       "      <td>[\"Top stories\", \"Cyber Security\", \"Huawei cave...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source                  timestamp  \\\n",
       "5765  reuters 2018-12-07 10:00:48.453100   \n",
       "5763     cnbc 2018-12-07 10:00:48.453100   \n",
       "5764       ft 2018-12-07 10:00:48.453100   \n",
       "5822     cnbc 2018-12-07 11:00:48.314326   \n",
       "5823       ft 2018-12-07 11:00:48.314326   \n",
       "\n",
       "                                               headline  \n",
       "5765  [\"Huawei CFO to appear in Canada court\", \"A to...  \n",
       "5763  [\"Tech stocks lift European markets; oil price...  \n",
       "5764  [\"Top stories\", \"Cyber Security\", \"Huawei cave...  \n",
       "5822  [\"European markets rally after global sell-off...  \n",
       "5823  [\"Top stories\", \"Cyber Security\", \"Huawei cave...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('~/Downloads/headlines.csv', parse_dates=['timestamp'], index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peeking into what how our raw text data look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"Tech stocks lift European markets; oil prices slide ahead of OPEC meeting\", \"Asian stocks take a breather after days of declines\", \"Dow set to fall by 200 points at the open as sell-off continues\", \"Gold set for best week since August, US jobs data eyed\", \"Oil sinks as OPEC mulls Iran supply cut exemption, tries to get Russia on board\", \"Treasury yields muted ahead of nonfarm payrolls\", \"Dollar struggles on Fed pause talk ahead of jobs data\", \"Dollar struggles on Fed pause talk ahead of jobs data\", \"Bitcoin/USD Coinbase\", \"Iran seeks exemptions as OPEC awaits approval from Russia to impose production cuts\", \"A crunch Brexit vote is coming next week that could plunge the UK into fresh political chaos\", \"2 Hours Ago\", \"Bitcoin plunges 11 percent as December rout continues\", \"Eustance Huang\", \"3 Hours Ago\", \"John Bolton \\'knew in advance\\' about arrest of Huawei executive; official says Trump did not\", \"Dow rebounds from 780-point plunge, ends day just slightly lower on report Fed may pause hikes\", \"Christine Wang, Fred Imbert, Eustance Huang\", \"Facebook emails show EU must toughen regulation on social media \\'monopoly,\\' lawmaker says\", \"Ryan Browne\", \"56 Mins Ago\", \"Oil sinks as OPEC mulls Iran supply cut exemption, tries to get Russia on board\", \"19 Mins Ago\", \"How cable-car systems are revolutionizing public transport almost 12,000 feet above sea level\", \"Anmar Frangoul\", \"42 Mins Ago\", \"Anti-election meddling group makes A.I.-powered Trump impersonator to warn about \\'deepfakes\\'\", \"Ryan Browne\", \"2 Hours Ago\", \"93% of funding for European tech start-ups goes to all-male teams, new report finds\", \"Elizabeth Schulze\", \"3 Hours Ago\", \"Germany is about to find out who\\'s likely to replace Merkel as leader\", \"Holly Ellyatt\", \"Huawei appoints chairman as its acting CFO after executive\\'s arrest\", \"Matt Clinch\", \"2 Hours Ago\", \"How this 32-year-old high school dropout created Hong Kong\\'s first billion-dollar start-up\", \"Karen Gilchrist\", \"Japan said to plan to bar Huawei, ZTE from government procurement contracts\", \"Soul-searching is in order for Nissan\\'s board after Ghosn allegations, governance experts say\", \"Kelly Olsen\", \"3 Hours Ago\", \"Reid Hoffman on starting LinkedIn, how to scale a business and his life\\'s philosophy\", \"Lucy Handley\", \"HSBC ties to Huawei could complicate China trade talks even though the bank isn\\'t being investigated\", \"Kate Fazzini\", \"India and Pakistan turn to religious diplomacy as peace talks stall\", \"Nyshka Chandran\", \"South Korean peace efforts look \\'out of sync\\' with elimination of North Korean nukes\", \"Nyshka Chandran\", \"Markets are going haywire. Here\\'s why these sudden moves are here to stay\", \"Jamie Dimon says the market is getting ripped around thanks to the trade war\", \"Trump seen naming former Fox News personality as next UN ambassador\", \"Tesla General Counsel Todd Maron is leaving the company\", \"Lora Kolodny\", \"3 Hours Ago\", \"Asian stocks take a breather after days of declines\", \"Eustance Huang\", \"2 Hours Ago\", \"Police raids were not the fault of Deutsche Bank management, CFO says\", \"Elon Musk says Boring Company tunnel under LA will now open on Dec. 18\", \"Yen Nee Lee\", \"Tariffs have hit confidence, to slow US economy, says Fed\\'s Williams\", \"OPEC meeting ends with no decision on production levels\", \"Sam Meredith, Tom DiChristopher\", \"Noble Group\\'s $3.5 billion restructuring at risk as authorities block new listing\", \"Israel\\'s Lebanon border operation seen as a political move by embattled Netanyahu\", \"Natasha Turak\", \"The sell-off started with a mysterious plunge overnight that caused the exchange to halt futures\", \"Thomas Franck\", \"US crude sinks 2.7%, settling at $51.49, after OPEC delays decision on production cut levels\", \"Tom DiChristopher, Sam Meredith, Matt Clinch\", \"These are the 10 best firms to work for in France, according to Glassdoor\", \"Alexandra Gibbs\", \"US firms doing business in China now vulnerable after Huawei arrest: Former US trade advisor\", \"Michelle Fox\", \"There may be more at stake than just trade concessions in the US-China tariff battle\", \"Nyshka Chandran\", \"Shares of Apple suppliers crumble after lensmaker reports more than 25 percent decline in revenue\", \"Eustance Huang\", \"Sell-offs could be down to machines that control 80% of the US stock market, fund manager says\", \"Silvia Amaro\", \"Follow CNBC International\", \"Market MOVERS\"]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['headline'].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty raw, this cell contains all the headlines from one news source (e.g. CNBC) at a specific timestamp. Multiple news list are stored as a JSON string. Next step is to do some cleaning on the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = data['source'].tolist()\n",
    "timestamp = data['timestamp'].tolist()\n",
    "headline = data['headline'].tolist()\n",
    "\n",
    "expanded_source = []\n",
    "expanded_timestamp = []\n",
    "expanded_headline = []\n",
    "\n",
    "# in the original dataframe, each row contains multiple headlines as shown above.\n",
    "# we need to separate them out into one headline per row.\n",
    "\n",
    "for i, hl in enumerate(headline):\n",
    "    hl_list = json.loads(hl)\n",
    "    for j in range(len(hl_list)):\n",
    "        \n",
    "        # removing all texts that are not really headlines\n",
    "        # noticed we have some news headline string that are not\n",
    "        # really a headline, e.g. \"2 Hours Ago\", \"Kelly Olsen\". \n",
    "        # these are just irrelevant metadata scraped from the website.\n",
    "        # It is easy to notice all such text data are 30 characters\n",
    "        # or less, hence we could simply filter them out by string length.\n",
    "        \n",
    "        if len(hl_list[j]) < 30:\n",
    "            continue\n",
    "        expanded_source.append(source[i])\n",
    "        expanded_timestamp.append(timestamp[i])\n",
    "        expanded_headline.append(hl_list[j])\n",
    "\n",
    "data = pd.DataFrame({'source': expanded_source, 'timestamp': expanded_timestamp,\n",
    "                     'headline': expanded_headline})\n",
    "\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# preprocessing pipeline to clean the text\n",
    "def text_preprocess(text, return_str=True):\n",
    "    table = str.maketrans('', '', '!\"#&\\'()*+,./:;<=>?@[\\\\]^_`{|}~‘’')\n",
    "    text = word_tokenize(text)\n",
    "    text = [t.translate(table) for t in text]\n",
    "    text = [t for t in text if t not in stop_words]\n",
    "    text = [lemmatiser.lemmatize(t) for t in text]\n",
    "    text = [stemmer.stem(t) for t in text]\n",
    "    text = [t for t in text if t != '']\n",
    "    return ' '.join(text) if return_str else text\n",
    "\n",
    "data['processed'] = data['headline'].apply(text_preprocess)\n",
    "data['date'] = data['timestamp'].apply(lambda x: x.date().isoformat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load stock index data for the past 7 months period. I downloaded both S&P 500 and MSCI World Indices. For this exercise, I will focus on MSCI, result on SPX are quite similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>spx</th>\n",
       "      <th>mxwo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>2760.17</td>\n",
       "      <td>2041.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-02</td>\n",
       "      <td>2760.17</td>\n",
       "      <td>2041.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-03</td>\n",
       "      <td>2790.37</td>\n",
       "      <td>2066.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-04</td>\n",
       "      <td>2700.06</td>\n",
       "      <td>2016.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-05</td>\n",
       "      <td>2700.06</td>\n",
       "      <td>2008.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date      spx     mxwo\n",
       "0 2018-12-01  2760.17  2041.36\n",
       "1 2018-12-02  2760.17  2041.36\n",
       "2 2018-12-03  2790.37  2066.62\n",
       "3 2018-12-04  2700.06  2016.89\n",
       "4 2018-12-05  2700.06  2008.63"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_data = pd.read_csv('~/Downloads/index.csv', parse_dates=['date'])\n",
    "index_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data included weekend as well. We filter them out.\n",
    "index_data['weekday'] = index_data['date'].apply(lambda x: x.isoweekday() <= 5)\n",
    "index_data = index_data[index_data['weekday']]\n",
    "del index_data['weekday']\n",
    "\n",
    "# use MSCI World Index 1 day return as the indicator for market sentiment\n",
    "index = 'mxwo'\n",
    "target = 'mxwo_1d_ret'\n",
    "\n",
    "index_data[target] = np.log(index_data[index + ''].shift(-1) / index_data[index + ''])\n",
    "index_data.dropna(inplace=True)\n",
    "\n",
    "# helper function to convert a daily return into a label within \n",
    "# each label represents a 1% return interval\n",
    "def generate_label(dataframe, column, interval=.01):\n",
    "    def categorize(num):\n",
    "        cat = (math.ceil(abs(num) / interval) ) * (1 if num > 0 else -1)\n",
    "        # winsorize the data to (+/-)2% interval to \n",
    "        # prevent having outlier classes that\n",
    "        # only show up in training set or test set\n",
    "        return cat if abs(cat) <= 2 else 2 if cat > 0 else -2\n",
    "    dataframe[column] = dataframe[column].apply(categorize)\n",
    "\n",
    "generate_label(index_data, target)\n",
    "index_data['date'] = index_data['date'].apply(lambda x: x.date().isoformat())\n",
    "\n",
    "# merge headline dataframe with index data. Note weekend news are \n",
    "# dropped in inner join for simplicity.\n",
    "data = data.merge(index_data, on='date', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>headline</th>\n",
       "      <th>processed</th>\n",
       "      <th>date</th>\n",
       "      <th>spx</th>\n",
       "      <th>mxwo</th>\n",
       "      <th>mxwo_1d_ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reuters</td>\n",
       "      <td>2018-12-07 10:00:48.453100</td>\n",
       "      <td>Huawei CFO to appear in Canada court</td>\n",
       "      <td>huawei cfo appear canada court</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>2633.08</td>\n",
       "      <td>1965.24</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reuters</td>\n",
       "      <td>2018-12-07 10:00:48.453100</td>\n",
       "      <td>A top executive of China's Huawei Technologies...</td>\n",
       "      <td>A top execut china huawei technolog arrest can...</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>2633.08</td>\n",
       "      <td>1965.24</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reuters</td>\n",
       "      <td>2018-12-07 10:00:48.453100</td>\n",
       "      <td>Huawei appoints chairman as acting CFO</td>\n",
       "      <td>huawei appoint chairman act cfo</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>2633.08</td>\n",
       "      <td>1965.24</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reuters</td>\n",
       "      <td>2018-12-07 10:00:48.453100</td>\n",
       "      <td>Japan to shun Huawei, ZTE equipment</td>\n",
       "      <td>japan shun huawei zte equip</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>2633.08</td>\n",
       "      <td>1965.24</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reuters</td>\n",
       "      <td>2018-12-07 10:00:48.453100</td>\n",
       "      <td>TV: Trump was unaware of arrest - officials</td>\n",
       "      <td>TV trump unawar arrest - offici</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>2633.08</td>\n",
       "      <td>1965.24</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source                  timestamp  \\\n",
       "0  reuters 2018-12-07 10:00:48.453100   \n",
       "1  reuters 2018-12-07 10:00:48.453100   \n",
       "2  reuters 2018-12-07 10:00:48.453100   \n",
       "3  reuters 2018-12-07 10:00:48.453100   \n",
       "4  reuters 2018-12-07 10:00:48.453100   \n",
       "\n",
       "                                            headline  \\\n",
       "0               Huawei CFO to appear in Canada court   \n",
       "1  A top executive of China's Huawei Technologies...   \n",
       "2             Huawei appoints chairman as acting CFO   \n",
       "3                Japan to shun Huawei, ZTE equipment   \n",
       "4        TV: Trump was unaware of arrest - officials   \n",
       "\n",
       "                                           processed        date      spx  \\\n",
       "0                     huawei cfo appear canada court  2018-12-07  2633.08   \n",
       "1  A top execut china huawei technolog arrest can...  2018-12-07  2633.08   \n",
       "2                    huawei appoint chairman act cfo  2018-12-07  2633.08   \n",
       "3                        japan shun huawei zte equip  2018-12-07  2633.08   \n",
       "4                    TV trump unawar arrest - offici  2018-12-07  2633.08   \n",
       "\n",
       "      mxwo  mxwo_1d_ret  \n",
       "0  1965.24           -1  \n",
       "1  1965.24           -1  \n",
       "2  1965.24           -1  \n",
       "3  1965.24           -1  \n",
       "4  1965.24           -1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176721, 8)\n",
      "2018-12-07\n",
      "2019-07-08\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data['date'].min())\n",
    "print(data['date'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "176721 headlines for the period from 2018-12-7 to 2019-7-8 captured. I will sample 30000 news for the purpose of this exercise. 20000 will be used for training and the remaining 10000 used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = np.random.choice(data.shape[0], size=30000, replace=False)\n",
    "train = data.loc[sample_index[:20000], :]\n",
    "test = data.loc[sample_index[20000:], :]\n",
    "\n",
    "# sampling without look-ahead bias\n",
    "# train = data.iloc[:100000, :].sample(10000)\n",
    "# test = data.iloc[100000:, :].sample(10000)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "# fit tokenizer on the whole corpus of text. Maybe restrict this to \n",
    "# training set corpus to avoid look-ahead bias?\n",
    "tokenizer.fit_on_texts(data['processed'].tolist())\n",
    "\n",
    "train_x = train['processed'].tolist()\n",
    "train_x = tokenizer.texts_to_sequences(train_x)\n",
    "headline_len = [len(vector) for vector in train_x]\n",
    "max_len = int(np.percentile(headline_len, 80))         # take the 80th percentile length\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "train_x = pad_sequences(train_x, maxlen=max_len, padding='pre', truncating='post')\n",
    "train_y = to_categorical(train[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of training, I will use a LSTM RNN to do this classification exercise. Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/hy/anaconda3/envs/hans36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/hy/anaconda3/envs/hans36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/hy/anaconda3/envs/hans36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      " - 16s - loss: 0.8299 - acc: 0.5948\n",
      "Epoch 2/30\n",
      " - 15s - loss: 0.6410 - acc: 0.6530\n",
      "Epoch 3/30\n",
      " - 15s - loss: 0.5495 - acc: 0.7337\n",
      "Epoch 4/30\n",
      " - 14s - loss: 0.4783 - acc: 0.7814\n",
      "Epoch 5/30\n",
      " - 13s - loss: 0.4212 - acc: 0.8144\n",
      "Epoch 6/30\n",
      " - 13s - loss: 0.3752 - acc: 0.8374\n",
      "Epoch 7/30\n",
      " - 13s - loss: 0.3366 - acc: 0.8554\n",
      "Epoch 8/30\n",
      " - 14s - loss: 0.3037 - acc: 0.8688\n",
      "Epoch 9/30\n",
      " - 13s - loss: 0.2761 - acc: 0.8791\n",
      "Epoch 10/30\n",
      " - 13s - loss: 0.2517 - acc: 0.8909\n",
      "Epoch 11/30\n",
      " - 13s - loss: 0.2361 - acc: 0.8962\n",
      "Epoch 12/30\n",
      " - 13s - loss: 0.2198 - acc: 0.9031\n",
      "Epoch 13/30\n",
      " - 14s - loss: 0.2030 - acc: 0.9131\n",
      "Epoch 14/30\n",
      " - 14s - loss: 0.1917 - acc: 0.9152\n",
      "Epoch 15/30\n",
      " - 14s - loss: 0.1827 - acc: 0.9184\n",
      "Epoch 16/30\n",
      " - 14s - loss: 0.1747 - acc: 0.9202\n",
      "Epoch 17/30\n",
      " - 14s - loss: 0.1683 - acc: 0.9229\n",
      "Epoch 18/30\n",
      " - 13s - loss: 0.1599 - acc: 0.9273\n",
      "Epoch 19/30\n",
      " - 13s - loss: 0.1562 - acc: 0.9262\n",
      "Epoch 20/30\n",
      " - 13s - loss: 0.1499 - acc: 0.9306\n",
      "Epoch 21/30\n",
      " - 13s - loss: 0.1452 - acc: 0.9298\n",
      "Epoch 22/30\n",
      " - 13s - loss: 0.1432 - acc: 0.9333\n",
      "Epoch 23/30\n",
      " - 17s - loss: 0.1402 - acc: 0.9325\n",
      "Epoch 24/30\n",
      " - 14s - loss: 0.1368 - acc: 0.9322\n",
      "Epoch 25/30\n",
      " - 13s - loss: 0.1328 - acc: 0.9343\n",
      "Epoch 26/30\n",
      " - 13s - loss: 0.1319 - acc: 0.9354\n",
      "Epoch 27/30\n",
      " - 13s - loss: 0.1290 - acc: 0.9353\n",
      "Epoch 28/30\n",
      " - 13s - loss: 0.1275 - acc: 0.9356\n",
      "Epoch 29/30\n",
      " - 13s - loss: 0.1259 - acc: 0.9353\n",
      "Epoch 30/30\n",
      " - 13s - loss: 0.1248 - acc: 0.9371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a499a6b38>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 500\n",
    "rnn = Sequential()\n",
    "rnn.add(Embedding(vocabulary_size, embedding_dim, input_length=max_len))\n",
    "rnn.add(SpatialDropout1D(0.2))\n",
    "rnn.add(LSTM(units=30, dropout=.2))\n",
    "rnn.add(Dense(train_y.shape[1], activation='softmax'))\n",
    "rnn.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "rnn.fit(train_x, train_y, batch_size=512, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training stats, looks like the process has converged successfully. Let's see how\n",
    "it performs on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headline level accuracy score 0.7074\n"
     ]
    }
   ],
   "source": [
    "# feed test set to feed into RNN\n",
    "test_x = test['processed'].tolist()\n",
    "test_x = tokenizer.texts_to_sequences(test_x)\n",
    "test_x = pad_sequences(test_x, maxlen=max_len, padding='pre', truncating='post')\n",
    "pred_y = rnn.predict(test_x)\n",
    "pred_y = np.argmax(pred_y, axis=1)\n",
    "\n",
    "# RNN's output doesn't use the class label we originally generated. So we \n",
    "# need to convert our generated label to the the class label using to_categorical() \n",
    "test_y = to_categorical(test[target])\n",
    "test_y = np.argmax(test_y, axis=1)\n",
    "\n",
    "headline = pd.DataFrame({'date': test['date'], 'headline': test['headline'], 'actual': test_y, 'predict': pred_y})\n",
    "headline.sort_values('date', inplace=True)\n",
    "\n",
    "# accuracy score on a per headline basis\n",
    "print('headline level accuracy score {}'.format(accuracy_score(headline['actual'], headline['predict'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each headline, the model is able to achieve an accuracy score of 71%. However, not all headlines are high relevance financial news headline. Some headlines are actually ads or articles on topics such as art, sports etc. These websites sometimes display non-financial articles (e.g. art, sport topics) on their front page. In order to come up with a prediction for the next day's return. We make use of all headlines we gathered on a day.\n",
    "\n",
    "Since our the activation function of our last layer is a softmax function which spits out probability value of belonging to each class, we could sum up the probability of each class of all articles for a day, pick the class with the largest value as our predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day level accuracy score 0.8355263157894737\n"
     ]
    }
   ],
   "source": [
    "pred_y = rnn.predict(test_x)\n",
    "#pred_y = scale(pred_y)\n",
    "pred_y = np.hstack((pred_y, test.loc[:, ['date']]))\n",
    "category_prob = pd.DataFrame(pred_y)\n",
    "category_prob.columns = [0, 1, 2, 'date']\n",
    "category_prob.sort_values('date', inplace=True)\n",
    "\n",
    "sum_prob = category_prob.groupby('date').sum()\n",
    "date = sum_prob.index\n",
    "label = np.argmax(sum_prob.values, axis=1)\n",
    "sum_prob = pd.DataFrame({'date': date, 'predict': label})\n",
    "sum_prob.set_index('date', inplace=True)\n",
    "y_label = headline.groupby('date').mean()['actual']\n",
    "result = sum_prob.join(y_label)\n",
    "\n",
    "# accuracy score on a per day basis\n",
    "print('day level accuracy score {}'.format(accuracy_score(result['actual'], result['predict'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, this drastically improves the accuracy rate to a 94%. That is out of my expectation. Let's have a look at how our prediction looks like on a per-headline basis and daily basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>actual</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>154066</th>\n",
       "      <td>2019-05-23</td>\n",
       "      <td>Facebook will not allow marijuana sales on its...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114897</th>\n",
       "      <td>2019-03-06</td>\n",
       "      <td>This company may be first big Chinese IPO of 2...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109675</th>\n",
       "      <td>2019-02-22</td>\n",
       "      <td>Queues of up to 15,000 people could stretch fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70398</th>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>The National Butterfly Center in Hidalgo Count...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98102</th>\n",
       "      <td>2019-02-05</td>\n",
       "      <td>Michigan’s investment arm bought Intel and IBM...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37154</th>\n",
       "      <td>2018-12-26</td>\n",
       "      <td>Want a free country house in Japan? They're gi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37589</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>'The worst is yet to come': Experts say a glob...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40074</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>In Japan, a scramble for new workers disrupts ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79601</th>\n",
       "      <td>2019-01-15</td>\n",
       "      <td>Markets need not be worried about China's econ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36534</th>\n",
       "      <td>2018-12-26</td>\n",
       "      <td>Niche markets in 2018: the bad and the bountiful</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130208</th>\n",
       "      <td>2019-04-05</td>\n",
       "      <td>Boeing says it found second software flaw on 7...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159541</th>\n",
       "      <td>2019-06-03</td>\n",
       "      <td>It is dangerous to underestimate Nancy Pelosi</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5330</th>\n",
       "      <td>2018-12-12</td>\n",
       "      <td>President bickers with top congressional Democ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122556</th>\n",
       "      <td>2019-03-21</td>\n",
       "      <td>Dad who used $250,000 worth of Facebook shares...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147772</th>\n",
       "      <td>2019-05-10</td>\n",
       "      <td>Shares tumble after-hours as cyber security co...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93181</th>\n",
       "      <td>2019-01-29</td>\n",
       "      <td>Reuters TV: U.S. charges Huawei with fraud</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50037</th>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>American farmers brace for more pain as Pacifi...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84552</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>Nearly 75% of users did not know about the soc...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149524</th>\n",
       "      <td>2019-05-14</td>\n",
       "      <td>Mark Hulbert: Why the ‘flash crash’ nine years...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20823</th>\n",
       "      <td>2018-12-19</td>\n",
       "      <td>Bond market’s inflation gauge slides to lowest...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date                                           headline  actual  \\\n",
       "154066  2019-05-23  Facebook will not allow marijuana sales on its...       1   \n",
       "114897  2019-03-06  This company may be first big Chinese IPO of 2...       2   \n",
       "109675  2019-02-22  Queues of up to 15,000 people could stretch fo...       1   \n",
       "70398   2019-01-10  The National Butterfly Center in Hidalgo Count...       2   \n",
       "98102   2019-02-05  Michigan’s investment arm bought Intel and IBM...       2   \n",
       "37154   2018-12-26  Want a free country house in Japan? They're gi...       1   \n",
       "37589   2018-12-27  'The worst is yet to come': Experts say a glob...       1   \n",
       "40074   2018-12-27  In Japan, a scramble for new workers disrupts ...       1   \n",
       "79601   2019-01-15  Markets need not be worried about China's econ...       1   \n",
       "36534   2018-12-26   Niche markets in 2018: the bad and the bountiful       1   \n",
       "130208  2019-04-05  Boeing says it found second software flaw on 7...       1   \n",
       "159541  2019-06-03      It is dangerous to underestimate Nancy Pelosi       2   \n",
       "5330    2018-12-12  President bickers with top congressional Democ...       2   \n",
       "122556  2019-03-21  Dad who used $250,000 worth of Facebook shares...       1   \n",
       "147772  2019-05-10  Shares tumble after-hours as cyber security co...       1   \n",
       "93181   2019-01-29         Reuters TV: U.S. charges Huawei with fraud       2   \n",
       "50037   2019-01-02  American farmers brace for more pain as Pacifi...       1   \n",
       "84552   2019-01-17  Nearly 75% of users did not know about the soc...       2   \n",
       "149524  2019-05-14  Mark Hulbert: Why the ‘flash crash’ nine years...       1   \n",
       "20823   2018-12-19  Bond market’s inflation gauge slides to lowest...       1   \n",
       "\n",
       "        predict  \n",
       "154066        1  \n",
       "114897        1  \n",
       "109675        1  \n",
       "70398         1  \n",
       "98102         1  \n",
       "37154         1  \n",
       "37589         2  \n",
       "40074         1  \n",
       "79601         1  \n",
       "36534         1  \n",
       "130208        2  \n",
       "159541        1  \n",
       "5330          2  \n",
       "122556        1  \n",
       "147772        2  \n",
       "93181         2  \n",
       "50037         2  \n",
       "84552         1  \n",
       "149524        2  \n",
       "20823         1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-12-11</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-28</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-14</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-19</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-27</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-05</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-23</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-26</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-22</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-07</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-22</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-28</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-24</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-19</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-17</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-25</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-25</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-12</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predict  actual\n",
       "date                       \n",
       "2018-12-11        2     2.0\n",
       "2019-03-28        1     1.0\n",
       "2019-03-14        1     1.0\n",
       "2019-06-19        2     2.0\n",
       "2019-02-27        2     2.0\n",
       "2019-03-05        1     2.0\n",
       "2019-04-01        1     2.0\n",
       "2019-01-23        1     1.0\n",
       "2019-03-26        1     2.0\n",
       "2019-05-22        1     1.0\n",
       "2019-05-07        2     2.0\n",
       "2019-03-22        1     2.0\n",
       "2019-05-28        2     2.0\n",
       "2019-04-24        1     2.0\n",
       "2019-02-19        1     1.0\n",
       "2019-01-17        1     2.0\n",
       "2019-01-04        1     1.0\n",
       "2019-01-25        1     2.0\n",
       "2019-03-25        1     1.0\n",
       "2019-03-12        1     1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result looks impressive. However, don't forget we generate our training and test \n",
    "set using random sampling on data across the entire 7 months period. This to some degree\n",
    "suffered from look-ahead basis. headlines from the same period normally have similar word\n",
    "distribution, in the beginning of Jan 2019, the market saw a wave of rebound. Having seen in training set words like \"surge, rebound, recover\" are associated with a very positive label, it is natural the classifier will link headlines with similar meaning to positive label. \n",
    "\n",
    "In the training/test data generation part,I commented out another way to generate train/test set without look ahead bias. Under this setting, the accuracy rate drops to around 50%! Will\n",
    "longer history help? Since we only have 7 months of data to train on, probably a long 10 year \n",
    "series span across different market sentiment cycle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
